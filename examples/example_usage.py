#!/usr/bin/env python3
"""
Example usage of the high-performance Cython Redis wrapper.
Demonstrates threading, async operations, and stream processing.
"""

import asyncio
import threading
import time
from typing import Dict, Any
import logging

# Import our high-performance Redis wrapper
from optimized_redis import OptimizedRedis as HighPerformanceRedis
# ThreadedStreamConsumer not implemented yet
ThreadedStreamConsumer = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def basic_operations_example():
    """Demonstrate basic Redis operations with high performance"""
    print("=== Basic Operations Example ===")

    with HighPerformanceRedis(max_workers=4) as redis:
        # Basic key-value operations
        redis.set("example:key", "Hello from Cython Redis!")
        value = redis.get("example:key")
        print(f"Retrieved value: {value}")

        # Publish to a channel
        subscribers = redis.publish("example:channel", "Test message")
        print(f"Published to {subscribers} subscribers")

        # Stream operations
        stream_name = "example:stream"
        message_id = redis.xadd(stream_name, {
            "user": "alice",
            "message": "Hello from high-performance Redis!",
            "timestamp": str(time.time())
        })
        print(f"Added message to stream: {message_id}")

        # Read from stream
        messages = redis.xread({stream_name: "0"}, count=1)
        for stream, msg_id, data in messages:
            print(f"Read from stream {stream}: {data}")


async def async_operations_example():
    """Demonstrate asynchronous Redis operations"""
    print("\n=== Async Operations Example ===")

    redis = HighPerformanceRedis(max_workers=4)

    try:
        # Async operations
        await redis.set_async("async:key", "Async value")
        value = await redis.get_async("async:key")
        print(f"Async retrieved value: {value}")

        # Async stream operations
        stream_name = "async:stream"
        message_id = await redis.xadd_async(stream_name, {
            "type": "async_message",
            "content": "This was sent asynchronously",
            "thread_id": str(threading.get_ident())
        })
        print(f"Async added message: {message_id}")

        messages = await redis.xread_async({stream_name: "0"}, count=1)
        for stream, msg_id, data in messages:
            print(f"Async read from stream: {data}")

    finally:
        redis.close()


def threaded_operations_example():
    """Demonstrate C-level threaded Redis operations for high throughput"""
    print("\n=== C-Level Threaded Operations Example ===")

    with HighPerformanceRedis(max_workers=8) as redis:
        # Submit multiple operations to C thread pool (fire-and-forget)
        print("Submitting 10 operations to C thread pool...")
        for i in range(10):
            redis.set_threaded(f"threaded:key:{i}", f"Value {i}")

        # Give C threads time to complete
        import time
        time.sleep(0.1)

        # Verify all keys were set
        print("Verifying operations completed...")
        for i in range(10):
            value = redis.get(f"threaded:key:{i}")
            print(f"Verified key {i}: {value}")

        print("‚úì All C-level threaded operations completed successfully")


def stream_consumer_group_example():
    """Demonstrate Redis Streams with consumer groups and dead letter queues"""
    print("\n=== Redis Streams Consumer Group Example ===")

    with HighPerformanceRedis() as redis:
        stream_name = "orders:stream"
        group_name = "order_processors"
        consumer_name = "worker_1"

        # Create consumer group
        consumer_group = StreamConsumerGroup(redis, stream_name, group_name, consumer_name)

        if not consumer_group.create_group():
            print("Consumer group already exists or failed to create")

        # Register message handlers
        def handle_order(data, msg_id):
            order_id = data.get('order_id')
            amount = data.get('amount', 0)

            # Simulate processing
            if amount > 1000:  # Simulate failure for large orders
                return False  # This will trigger retry/dead letter logic

            print(f"‚úì Processed order {order_id} for ${amount}")
            return True

        def handle_payment(data, msg_id):
            payment_id = data.get('payment_id')
            print(f"‚úì Processed payment {payment_id}")
            return True

        consumer_group.register_handler('order', handle_order)
        consumer_group.register_handler('payment', handle_payment)

        # Add some test messages
        print("Adding test messages to stream...")
        redis.xadd(stream_name, {'type': 'order', 'order_id': '123', 'amount': 99.99})
        redis.xadd(stream_name, {'type': 'order', 'order_id': '124', 'amount': 1500})  # Will fail
        redis.xadd(stream_name, {'type': 'payment', 'payment_id': 'pay_456'})

        # Process messages
        processed = consumer_group.process_messages(count=10)
        print(f"Processed {processed} messages")

        # Check pending info
        pending_info = consumer_group.get_pending_info()
        print(f"Pending messages: {pending_info['pending_count']}")

        print("‚úì Stream consumer group example completed")


def worker_queue_example():
    """Demonstrate worker queue with priority and dead letter queues"""
    print("\n=== Worker Queue Example ===")

    with HighPerformanceRedis() as redis:
        # Create worker queue
        queue = WorkerQueue(redis, "email_jobs", use_streams=False)

        # Register job handlers
        def send_email(job):
            recipient = job['data'].get('email')
            subject = job['data'].get('subject')

            # Simulate sending email
            if recipient == 'fail@example.com':  # Simulate failure
                return False

            print(f"üìß Sent email to {recipient}: {subject}")
            return True

        def send_sms(job):
            phone = job['data'].get('phone')
            message = job['data'].get('message')
            print(f"üì± Sent SMS to {phone}: {message}")
            return True

        queue.register_job_handler('email', send_email)
        queue.register_job_handler('sms', send_sms)

        # Enqueue jobs with different priorities
        print("Enqueuing jobs...")
        queue.enqueue_job('email', {'email': 'user@example.com', 'subject': 'Welcome!'}, 'normal')
        queue.enqueue_job('email', {'email': 'vip@example.com', 'subject': 'VIP Welcome'}, 'high')
        queue.enqueue_job('email', {'email': 'fail@example.com', 'subject': 'This will fail'}, 'normal')
        queue.enqueue_job('sms', {'phone': '+1234567890', 'message': 'Your order shipped!'}, 'normal')

        # Process jobs
        processed = queue.process_jobs("worker_1", max_jobs=10)
        print(f"Processed {processed} jobs")

        # Check queue stats
        stats = queue.get_queue_stats()
        print(f"Queue stats: {stats}")

        print("‚úì Worker queue example completed")


def pubsub_hub_example():
    """Demonstrate advanced pub/sub with pattern matching"""
    print("\n=== Pub/Sub Hub Example ===")

    with HighPerformanceRedis() as redis:
        # Create pub/sub hub
        hub = PubSubHub(redis)

        # Define callbacks
        def user_callback(channel, data):
            print(f"üë§ User event on {channel}: {data}")

        def system_callback(channel, data):
            print(f"‚öôÔ∏è System event on {channel}: {data}")

        def pattern_callback(pattern, data):
            print(f"üîç Pattern match {pattern}: {data}")

        # Subscribe to specific channels
        hub.subscribe('user:login', user_callback)
        hub.subscribe('user:logout', user_callback)

        # Subscribe to patterns
        hub.psubscribe('system:*', system_callback)

        # Start the hub
        hub.start()

        # Publish messages
        print("Publishing messages...")
        hub.publish('user:login', {'user_id': 123, 'ip': '192.168.1.1'})
        hub.publish('user:logout', {'user_id': 123, 'session_time': 3600})
        hub.publish('system:backup', {'status': 'completed', 'size': '1.2GB'})
        hub.publish('system:alert', {'level': 'warning', 'message': 'High CPU usage'})

        # Give time for messages to be processed
        import time
        time.sleep(0.1)

        # Get channel info
        info = hub.get_channel_info()
        print(f"Channel info: {len(info['channels'])} active channels")

        # Stop the hub
        hub.stop()

        print("‚úì Pub/Sub hub example completed")


def keyspace_notifications_example():
    """Demonstrate keyspace notifications and watchers"""
    print("\n=== Keyspace Notifications Example ===")

    with HighPerformanceRedis() as redis:
        # Create keyspace watcher
        watcher = KeyspaceWatcher(redis)

        # Enable keyspace notifications
        if watcher.enable_notifications('AKE'):
            print("‚úì Keyspace notifications enabled")

            # Register event callbacks
            def on_key_set(key, event):
                print(f"üîë Key set: {key} (event: {event})")

            def on_key_del(key, event):
                print(f"üóëÔ∏è Key deleted: {key} (event: {event})")

            def on_key_expire(key, event):
                print(f"‚è∞ Key expired: {key} (event: {event})")

            watcher.on_event_type('set', on_key_set)
            watcher.on_event_type('del', on_key_del)
            watcher.on_event_type('expire', on_key_expire)

            # Watch specific keys
            watcher.on_key_event('test:*', lambda k, e: print(f"üìä Test key event: {k} -> {e}"))

            # Start watching
            consumer = watcher.start_watching()

            # Generate some events
            print("Generating key events...")
            redis.set('test:user:123', 'John Doe')
            redis.set('session:abc', 'data', ex=2)  # Expires in 2 seconds
            redis.delete('test:user:123')

            # Wait for expiration
            import time
            time.sleep(3)

            # Stop watching
            consumer.stop()

        else:
            print("‚úó Failed to enable keyspace notifications")

        print("‚úì Keyspace notifications example completed")


def distributed_locks_example():
    """Demonstrate distributed locks and synchronizers"""
    print("\n=== Distributed Locks & Synchronizers Example ===")

    with HighPerformanceRedis() as redis:
        # Distributed Lock
        lock = DistributedLock(redis, "resource_lock", lease_time=10000)

        print("Testing distributed lock...")
        if lock.try_lock(wait_time=2000):
            print("‚úì Acquired distributed lock")
            # Do some work...
            import time
            time.sleep(0.1)
            lock.unlock()
            print("‚úì Released distributed lock")
        else:
            print("‚úó Failed to acquire lock")

        # Read-Write Lock
        rw_lock = ReadWriteLock(redis, "rw_resource")

        print("Testing read-write lock...")
        if rw_lock.read_lock():
            print("‚úì Acquired read lock")
            rw_lock.read_unlock()
            print("‚úì Released read lock")

        if rw_lock.write_lock():
            print("‚úì Acquired write lock")
            rw_lock.write_unlock()
            print("‚úì Released write lock")

        # Semaphore
        semaphore = Semaphore(redis, "worker_slots", permits=3)

        print("Testing semaphore...")
        if semaphore.acquire():
            print("‚úì Acquired semaphore permit")
            print(f"Available permits: {semaphore.available_permits()}")
            semaphore.release()
            print("‚úì Released semaphore permit")

        # Distributed Counter
        counter = DistributedCounter(redis, "request_count")

        print("Testing distributed counter...")
        print(f"Initial value: {counter.get()}")
        print(f"Increment: {counter.increment()}")
        print(f"Add 5: {counter.add(5)}")
        print(f"Final value: {counter.get()}")

        print("‚úì Distributed locks example completed")


def local_cache_example():
    """Demonstrate local cache with Redis backing"""
    print("\n=== Local Cache with Redis Backing Example ===")

    with HighPerformanceRedis() as redis:
        # Create local cache
        cache = LocalCache(redis, namespace="app", max_size=100, ttl=30)

        print("Testing local cache...")

        # Put some data
        cache.put("user:123", {"name": "John", "email": "john@example.com"})
        cache.put("config:theme", "dark")

        # Get data (should hit cache)
        user_data = cache.get("user:123")
        theme = cache.get("config:theme")

        print(f"‚úì Cached user data: {user_data}")
        print(f"‚úì Cached theme: {theme}")

        # Test TTL
        import time
        time.sleep(1)

        # Data should still be fresh
        fresh_data = cache.get("user:123")
        print(f"‚úì Fresh data (TTL not expired): {fresh_data}")

        # Invalidate
        cache.invalidate("user:123")
        invalidated = cache.get("user:123")
        print(f"‚úì Invalidated data: {invalidated}")

        print("‚úì Local cache example completed")


def live_object_example():
    """Demonstrate live object service"""
    print("\n=== Live Object Service Example ===")

    with HighPerformanceRedis() as redis:
        # Define a simple class
        class User:
            def __init__(self, name="", email="", age=0):
                self.name = name
                self.email = email
                self.age = age

            def __str__(self):
                return f"User(name='{self.name}', email='{self.email}', age={self.age})"

        # Create live object
        live_user = LiveObject(redis, User, "user:live:123")

        # Get object (loads from Redis or creates new)
        user = live_user.get()
        print(f"‚úì Live object: {user}")

        # Modify object
        user.name = "Jane Doe"
        user.email = "jane@example.com"
        user.age = 30

        # Save to Redis
        live_user.save()
        print(f"‚úì Saved live object: {user}")

        # Create another instance and load
        live_user2 = LiveObject(redis, User, "user:live:123")
        user2 = live_user2.get()
        print(f"‚úì Loaded live object: {user2}")

        # Delete object
        live_user.delete()

        print("‚úì Live object example completed")


def redisson_pro_features_example():
    """Demonstrate Redisson PRO-like features"""
    print("\n=== Redisson PRO-like Features Showcase ===")

    with HighPerformanceRedis() as redis:
        print("üöÄ CyRedis - Redisson PRO equivalent for Python")
        print("=" * 60)

        # 1. Distributed Objects (Hashes)
        print("1. Distributed Objects (Hashes):")
        redis.hmset("user:profile", {
            "name": "John Doe",
            "email": "john@example.com",
            "role": "admin"
        })
        profile = redis.hgetall("user:profile")
        print(f"   ‚úì Hash object: {profile}")

        # 2. Distributed Collections (Sets & Sorted Sets)
        print("2. Distributed Collections:")
        redis.sadd("active_users", ["user1", "user2", "user3"])
        active_count = redis.scard("active_users")
        print(f"   ‚úì Set with {active_count} members")

        redis.zadd("leaderboard", {"user1": 100, "user2": 150, "user3": 75})
        top_players = redis.zrevrange("leaderboard", 0, 2, withscores=True)
        print(f"   ‚úì Sorted set leaderboard: {top_players}")

        # 3. Distributed Locks
        print("3. Distributed Locks:")
        lock = DistributedLock(redis, "critical_section")
        if lock.try_lock():
            print("   ‚úì Acquired distributed lock")
            lock.unlock()
            print("   ‚úì Released distributed lock")

        # 4. Reliable Messaging (Streams)
        print("4. Reliable Messaging (Streams):")
        msg_id = redis.xadd("events", {"type": "user_action", "action": "login"})
        print(f"   ‚úì Added message to stream: {msg_id}")

        # 5. Local Cache
        print("5. Local Cache:")
        cache = LocalCache(redis, "cache", max_size=100, ttl=60)
        cache.put("frequent_data", {"hits": 1000, "last_access": "2024-01-01"})
        cached_data = cache.get("frequent_data")
        print(f"   ‚úì Cached data: {cached_data}")

        # 6. Live Object Service
        print("6. Live Object Service:")
        class Config:
            def __init__(self, debug=False, timeout=30):
                self.debug = debug
                self.timeout = timeout

        live_config = LiveObject(redis, Config, "app:config")
        config = live_config.get()
        config.debug = True
        config.timeout = 60
        live_config.save()
        print("   ‚úì Live object saved to Redis")

        # 7. Worker Queues
        print("7. Worker Queues:")
        queue = WorkerQueue(redis, "tasks")
        job_id = queue.enqueue_job("send_email", {"to": "test@example.com"}, "normal")
        print(f"   ‚úì Enqueued job: {job_id}")

        stats = queue.get_queue_stats()
        print(f"   ‚úì Queue stats: {stats}")

        # 8. Keyspace Notifications
        print("8. Keyspace Notifications:")
        watcher = KeyspaceWatcher(redis)
        if watcher.enable_notifications('A'):
            print("   ‚úì Keyspace notifications enabled")

        print("\nüéâ All Redisson PRO equivalent features demonstrated!")
        print("‚úì Distributed objects, collections, locks")
        print("‚úì Reliable messaging with streams")
        print("‚úì Local caching with Redis backing")
        print("‚úì Live object service")
        print("‚úì Worker queues and job processing")
        print("‚úì Keyspace notifications and watchers")

        print("\n‚ú® CyRedis provides Python equivalents of all major Redisson PRO features!")


async def uvloop_async_example():
    """Demonstrate uvloop-powered async Redis operations"""
    print("\n=== uvloop Async Redis Example ===")

    # Enable uvloop for better performance
    try:
        # Async features not implemented yet
        enable_uvloop = lambda: None
        HighPerformanceAsyncRedis = None
        is_uvloop_enabled = lambda: False

        enabled = enable_uvloop()
        print(f"‚úì uvloop enabled: {enabled}")

        async with HighPerformanceAsyncRedis(use_uvloop=True) as redis:
            print(f"‚úì Using uvloop: {is_uvloop_enabled()}")

            # Perform async operations with uvloop acceleration
            await redis.set("uvloop:key", "uvloop_value")
            value = await redis.get("uvloop:key")
            print(f"‚úì Async GET result: {value}")

            # Concurrent operations
            tasks = []
            for i in range(5):
                tasks.append(redis.set(f"uvloop:batch:{i}", f"value_{i}"))
                tasks.append(redis.incr("uvloop:counter"))

            results = await asyncio.gather(*tasks)
            print(f"‚úì Concurrent operations completed: {len(results)} operations")

            # Script execution
            script_result = await redis.execute_script(
                "return redis.call('GET', KEYS[1])",
                keys=["uvloop:key"],
                args=[]
            )
            print(f"‚úì Async script result: {script_result}")

            # Pipeline operations
            commands = [
                ("set", "uvloop:pipeline:1", "value1"),
                ("set", "uvloop:pipeline:2", "value2"),
                ("get", "uvloop:pipeline:1"),
                ("get", "uvloop:pipeline:2"),
            ]

            pipeline_results = await redis.pipeline_execute(commands)
            print(f"‚úì Pipeline results: {pipeline_results}")

    except ImportError:
        print("‚ö†Ô∏è uvloop not available, falling back to standard asyncio")
        print("Install uvloop for better performance: pip install uvloop")

        # Fallback demonstration
        async with HighPerformanceAsyncRedis(use_uvloop=False) as redis:
            await redis.set("fallback:key", "fallback_value")
            value = await redis.get("fallback:key")
            print(f"‚úì Fallback async result: {value}")

    print("‚úì uvloop async example completed")


def lua_scripts_example():
    """Demonstrate comprehensive Lua script functionality"""
    print("\n=== Lua Scripts Library Example ===")

    with HighPerformanceRedis() as redis:
        scripts = ScriptHelper(redis)

        # Rate Limiting Examples
        print("üìä Testing Rate Limiting...")

        # Sliding window rate limiting
        user_key = "user:123:requests"
        allowed = scripts.rate_limit_sliding_window(user_key, limit=5, window_seconds=60)
        print(f"‚úì Sliding window rate limit (5/60s): {'ALLOWED' if allowed else 'BLOCKED'}")

        # Token bucket rate limiting
        api_key = "api:key:abc"
        has_tokens = scripts.rate_limit_token_bucket(api_key, capacity=10, refill_rate=1.0)
        print(f"‚úì Token bucket rate limit (10 tokens, 1/sec refill): {'ALLOWED' if has_tokens else 'BLOCKED'}")

        # Distributed Locks
        print("\nüîí Testing Distributed Locks...")
        lock_key = "resource:lock"
        lock_value = "worker:1"

        locked = scripts.acquire_lock(lock_key, lock_value, ttl_ms=5000)
        print(f"‚úì Lock acquired: {locked}")

        if locked:
            # Do some work...
            released = scripts.release_lock(lock_key, lock_value)
            print(f"‚úì Lock released: {released}")

        # Bounded Counter
        print("\nüî¢ Testing Bounded Counter...")
        counter_key = "inventory:widget"
        new_count = scripts.bounded_increment(counter_key, increment=5, min_val=0, max_val=100)
        print(f"‚úì Bounded counter (0-100): {new_count}")

        # Priority Queue
        print("\nüìã Testing Priority Queue...")
        queue_key = "jobs:high_priority"

        # Add jobs with different priorities (lower number = higher priority)
        scripts.push_priority_queue(queue_key, priority=1, item="urgent_job")
        scripts.push_priority_queue(queue_key, priority=3, item="normal_job")
        scripts.push_priority_queue(queue_key, priority=2, item="important_job")

        # Pop highest priority jobs
        urgent_jobs = scripts.pop_priority_queue(queue_key, count=2)
        print(f"‚úì Priority queue popped: {urgent_jobs}")

        # Batch Operations
        print("\nüì¶ Testing Batch Operations...")
        batch_data = {
            "session:1": "user_123",
            "session:2": "user_456",
            "cache:key1": "value1",
            "cache:key2": "value2"
        }

        set_count = scripts.batch_set_with_ttl(batch_data, ttl_seconds=300)
        print(f"‚úì Batch set with TTL: {set_count} keys")

        # Atomic set operations
        set_key = "user:tags:123"
        added = scripts.set_add_multiple(set_key, ["admin", "moderator", "premium"])
        print(f"‚úì Added to set: {added} new items")

        removed = scripts.set_remove_multiple(set_key, ["moderator", "guest"])
        print(f"‚úì Removed from set: {removed} items")

        # Bloom Filter
        print("\nüå∏ Testing Bloom Filter...")
        bloom_key = "email:filter"

        # Add emails to filter
        added1 = scripts.bloom_add(bloom_key, "user@example.com")
        added2 = scripts.bloom_add(bloom_key, "admin@example.com")
        print(f"‚úì Added to bloom filter: {added1 + added2} items")

        # Check membership
        exists1 = scripts.bloom_check(bloom_key, "user@example.com")
        exists2 = scripts.bloom_check(bloom_key, "unknown@example.com")
        print(f"‚úì Bloom filter check - known: {exists1}, unknown: {exists2}")

        # Semaphore
        print("\nüö¶ Testing Semaphore...")
        sem_key = "database:connections"

        acquired = scripts.semaphore_acquire(sem_key, permits=1, timeout_seconds=0)
        print(f"‚úì Semaphore acquired: {acquired}")

        if acquired:
            # Simulate work
            released_count = scripts.semaphore_release(sem_key, max_permits=5)
            print(f"‚úì Semaphore released, count: {released_count}")

        # Time Series
        print("\nüìà Testing Time Series...")
        series_key = "sensor:temperature"

        import time
        timestamp = int(time.time() * 1000)  # milliseconds

        size1 = scripts.time_series_add(series_key, timestamp, "23.5", retention_seconds=3600)
        size2 = scripts.time_series_add(series_key, timestamp + 1000, "24.1", retention_seconds=3600)
        print(f"‚úì Added to time series: {size2} points")

        # Query range
        data_points = scripts.time_series_range(series_key, timestamp, timestamp + 2000)
        print(f"‚úì Time series range query: {len(data_points)} points")

        # Atomic Configuration Update
        print("\n‚öôÔ∏è Testing Atomic Configuration...")
        config_key = "app:settings"

        changes = scripts.config_update_atomic(config_key, {
            "max_connections": "100",
            "timeout": "30",
            "debug": "false"
        })
        print(f"‚úì Configuration updated: {len(changes)} fields changed")

        # Leader Election
        print("\nüëë Testing Leader Election...")
        leader_key = "cluster:leader"
        candidate = "node:1"

        elected = scripts.leader_heartbeat(leader_key, candidate, ttl_seconds=30)
        print(f"‚úì Leader election: {'ELECTED' if elected else 'NOT ELECTED'}")

        is_leader = scripts.check_leader(leader_key, candidate)
        print(f"‚úì Is leader: {is_leader}")

    print("‚úì Lua scripts example completed")


def external_lua_scripts_example():
    """Demonstrate loading and using external Lua script files"""
    print("\n=== External Lua Scripts Example ===")

    with HighPerformanceRedis() as redis:
        script_manager = LuaScriptManager(redis)

        # Load rate limiter script from file
        print("üìú Loading external Lua scripts...")

        try:
            with open('lua_scripts/rate_limiter.lua', 'r') as f:
                rate_limiter_script = f.read()

            with open('lua_scripts/distributed_lock.lua', 'r') as f:
                lock_script = f.read()

            with open('lua_scripts/smart_cache.lua', 'r') as f:
                cache_script = f.read()

            with open('lua_scripts/job_queue.lua', 'r') as f:
                job_queue_script = f.read()

            # Register scripts
            script_manager.register_script('external_rate_limiter', rate_limiter_script)
            script_manager.register_script('external_lock', lock_script)
            script_manager.register_script('external_cache', cache_script)
            script_manager.register_script('external_queue', job_queue_script)

            print("‚úì Scripts loaded successfully")

        except FileNotFoundError as e:
            print(f"‚ö†Ô∏è Lua script files not found: {e}")
            print("Make sure you're running from the redis-stuff directory")
            return

        # Test the external rate limiter
        print("\nüö¶ Testing External Rate Limiter...")
        current_time = int(time.time() * 1000)

        for i in range(3):
            result = script_manager.execute_script(
                'external_rate_limiter',
                keys=['external:user:requests'],
                args=[5, 60000, current_time + (i * 1000), 10]  # 5 requests per minute, burst of 10
            )
            print(f"‚úì Request {i+1}: remaining capacity = {result}")

        # Test the external distributed lock
        print("\nüîê Testing External Distributed Lock...")
        lock_result = script_manager.execute_script(
            'external_lock',
            keys=['external:resource:lock'],
            args=['worker_1', 10000]  # 10 second TTL
        )
        print(f"‚úì Lock acquired: {lock_result}")

        # Test the external smart cache
        print("\nüíæ Testing External Smart Cache...")
        import time
        current_time = int(time.time())

        # Set cache entry
        set_result = script_manager.execute_script(
            'external_cache',
            keys=['external:cache:test', 'external:access:log', 'external:cache:stats'],
            args=['SET', current_time, 'cached_value', 300, 100]  # 5 min TTL, max 100 entries
        )
        print(f"‚úì Cache set: {set_result}")

        # Get cache entry
        get_result = script_manager.execute_script(
            'external_cache',
            keys=['external:cache:test', 'external:access:log', 'external:cache:stats'],
            args=['GET', current_time + 1]
        )
        print(f"‚úì Cache get: {get_result}")

        # Get cache statistics
        stats_result = script_manager.execute_script(
            'external_cache',
            keys=['external:cache:test', 'external:access:log', 'external:cache:stats'],
            args=['STATS', current_time + 2]
        )
        print(f"‚úì Cache stats: {stats_result}")

        # Test the external job queue
        print("\nüìã Testing External Job Queue...")
        queue_time = int(time.time())

        # Push a job
        push_result = script_manager.execute_script(
            'external_queue',
            keys=['external:jobs', 'external:processing', 'external:failed', 'external:dead'],
            args=['PUSH', queue_time, 'job_001', '{"task": "process_data", "priority": 1}', 5, 3, 0]
        )
        print(f"‚úì Job pushed: {push_result}")

        # Pop a job
        pop_result = script_manager.execute_script(
            'external_queue',
            keys=['external:jobs', 'external:processing', 'external:failed', 'external:dead'],
            args=['POP', queue_time + 1, 1]
        )
        print(f"‚úì Job popped: {pop_result}")

        # Get queue statistics
        queue_stats = script_manager.execute_script(
            'external_queue',
            keys=['external:jobs', 'external:processing', 'external:failed', 'external:dead'],
            args=['STATS', queue_time + 2]
        )
        print(f"‚úì Queue stats: {queue_stats}")

    print("‚úì External Lua scripts example completed")


def redisson_pro_features_example():
    """Demonstrate Redisson PRO equivalent features we now have"""
    print("\n=== Redisson PRO Equivalent Features ===")

    with HighPerformanceRedis() as redis:
        # Web Session Management
        print("üåê Testing Web Session Management...")
        session_manager = WebSessionManager(redis)

        session_id = "user_session_123"
        session_data = {
            "user_id": 123,
            "username": "john_doe",
            "permissions": ["read", "write"],
            "last_page": "/dashboard"
        }

        # Create session
        created = session_manager.create_session(session_id, session_data)
        print(f"‚úì Session created: {created}")

        # Get session data
        retrieved_data = session_manager.get_session(session_id)
        print(f"‚úì Session retrieved: {retrieved_data}")

        # Update session
        session_data["last_page"] = "/profile"
        updated = session_manager.update_session(session_id, session_data)
        print(f"‚úì Session updated: {updated}")

        # Get session info
        session_info = session_manager.get_session_info(session_id)
        print(f"‚úì Session info: {session_info}")

        # XA Transactions
        print("\nüîÑ Testing XA Transactions...")
        xa_manager = XATransactionManager(redis)

        tx_id = "tx_001"
        xa_manager.begin_transaction(tx_id)

        # Prepare transaction with operations
        operations = [
            {"type": "set", "key": "xa:key1", "value": "value1"},
            {"type": "set", "key": "xa:key2", "value": "value2"},
            {"type": "incr", "key": "xa:counter"}
        ]

        prepared = xa_manager.prepare_transaction(tx_id, operations)
        print(f"‚úì Transaction prepared: {prepared}")

        # Commit transaction
        committed = xa_manager.commit_transaction(tx_id)
        print(f"‚úì Transaction committed: {committed}")

        # Observability & Monitoring
        print("\nüìä Testing Observability & Monitoring...")
        observability = ObservabilityManager(redis)

        # Record some operations
        observability.record_operation("set", 1.5, success=True)
        observability.record_operation("get", 0.8, success=True)
        observability.record_operation("lua_script", 5.2, success=False)

        # Get metrics
        metrics = observability.get_operation_metrics("set")
        print(f"‚úì Operation metrics: {metrics}")

        # Health check
        health = observability.health_check()
        print(f"‚úì Health check status: {health['status']}")

        # Security Features
        print("\nüîí Testing Security Features...")
        security = SecurityManager(redis)

        # Encrypt and store password
        security.store_encrypted_password("database", "admin", "super_secret_password")

        # Retrieve and decrypt password
        decrypted = security.get_decrypted_password("database", "admin")
        print(f"‚úì Password encryption/decryption: {'‚úì' if decrypted == 'super_secret_password' else '‚úó'}")

        # Access control
        # First set up some permissions
        redis.sadd("security:permissions:user123", "read", "write")

        access_granted = security.validate_access("resource:data", "user123", "read")
        print(f"‚úì Access control: {'granted' if access_granted else 'denied'}")

        # Audit logging
        security.audit_log("data_access", "user123", "resource:data", success=True,
                          details={"operation": "read", "table": "users"})

        print("‚úì Redisson PRO equivalent features example completed")


def amqp_routing_example():
    """Demonstrate AMQP-style routing with exchanges and queues"""
    print("\n=== AMQP-Style Routing Example ===")

    with HighPerformanceRedis() as redis:
        # AMQPRouter not implemented yet
        AMQPRouter = None

        router = AMQPRouter(redis, namespace="demo")

        # Create exchanges of different types
        print("üì° Creating exchanges...")

        router.create_exchange("direct_logs", "direct")
        router.create_exchange("topic_logs", "topic")
        router.create_exchange("fanout_logs", "fanout")
        print("‚úì Created direct, topic, and fanout exchanges")

        # Create queues
        print("\nüìã Creating queues...")

        # Bind queues to direct exchange
        router.bind_queue("direct_logs", "direct_info", "info")
        router.bind_queue("direct_logs", "direct_error", "error")
        router.bind_queue("direct_logs", "direct_warning", "warning")

        # Bind queues to topic exchange
        router.bind_queue("topic_logs", "topic_kernel", "kernel.*")
        router.bind_queue("topic_logs", "topic_auth", "*.auth.*")
        router.bind_queue("topic_logs", "topic_all", "#")

        # Bind queues to fanout exchange
        router.bind_queue("fanout_logs", "fanout_audit", "")
        router.bind_queue("fanout_logs", "fanout_backup", "")
        print("‚úì Created and bound queues")

        # Publish messages to different exchanges
        print("\nüì§ Publishing messages...")

        # Direct exchange messages
        result1 = router.publish("direct_logs", "info", "System startup complete")
        result2 = router.publish("direct_logs", "error", "Database connection failed")
        print(f"‚úì Direct exchange: info routed to {len(result1['routed_queues'])} queues")
        print(f"‚úì Direct exchange: error routed to {len(result2['routed_queues'])} queues")

        # Topic exchange messages
        result3 = router.publish("topic_logs", "kernel.memory", "Memory usage high")
        result4 = router.publish("topic_logs", "user.auth.login", "User login successful")
        result5 = router.publish("topic_logs", "system.cron.backup", "Backup completed")
        print(f"‚úì Topic exchange: kernel.memory routed to {len(result3['routed_queues'])} queues")
        print(f"‚úì Topic exchange: user.auth.login routed to {len(result4['routed_queues'])} queues")
        print(f"‚úì Topic exchange: system.cron.backup routed to {len(result5['routed_queues'])} queues")

        # Fanout exchange messages
        result6 = router.publish("fanout_logs", "", "System health check")
        print(f"‚úì Fanout exchange: message routed to {len(result6['routed_queues'])} queues")

        # Consume messages from queues
        print("\nüì• Consuming messages...")

        # Consume from direct queues
        direct_info_msgs = router.consume("direct_info", 10)
        direct_error_msgs = router.consume("direct_error", 10)
        print(f"‚úì direct_info queue: {len(direct_info_msgs)} messages")
        print(f"‚úì direct_error queue: {len(direct_error_msgs)} messages")

        # Consume from topic queues
        topic_kernel_msgs = router.consume("topic_kernel", 10)
        topic_auth_msgs = router.consume("topic_auth", 10)
        topic_all_msgs = router.consume("topic_all", 10)
        print(f"‚úì topic_kernel queue: {len(topic_kernel_msgs)} messages")
        print(f"‚úì topic_auth queue: {len(topic_auth_msgs)} messages")
        print(f"‚úì topic_all queue: {len(topic_all_msgs)} messages")

        # Consume from fanout queues
        fanout_audit_msgs = router.consume("fanout_audit", 10)
        fanout_backup_msgs = router.consume("fanout_backup", 10)
        print(f"‚úì fanout_audit queue: {len(fanout_audit_msgs)} messages")
        print(f"‚úì fanout_backup queue: {len(fanout_backup_msgs)} messages")

        # Get exchange and queue information
        print("\n‚ÑπÔ∏è  Exchange and queue information...")

        direct_info = router.get_exchange_info("direct_logs")
        topic_info = router.get_exchange_info("topic_logs")
        fanout_info = router.get_exchange_info("fanout_logs")

        print(f"‚úì Direct exchange: {direct_info['bindings_count']} bindings")
        print(f"‚úì Topic exchange: {topic_info['bindings_count']} bindings")
        print(f"‚úì Fanout exchange: {fanout_info['bindings_count']} bindings")

        # Queue information
        queues = ["direct_info", "direct_error", "topic_kernel", "topic_auth", "fanout_audit"]
        for queue_name in queues:
            queue_info = router.get_queue_info(queue_name)
            print(f"‚úì Queue {queue_name}: {queue_info['message_count']} messages")

    print("‚úì AMQP-style routing example completed")


def cython_performance_example():
    """Demonstrate Cython-optimized operations for maximum performance"""
    print("\n=== Cython Performance Optimizations Example ===")

    with HighPerformanceRedis() as redis:
        # Test Cython-optimized operations
        print("üßÆ Testing Cython-optimized operations...")

        # Batch operations
        batch_ops = [
            ('set', 'cython:key1', 'value1'),
            ('set', 'cython:key2', 'value2'),
            ('set', 'cython:key3', 'value3'),
            ('mget', ['cython:key1', 'cython:key2', 'cython:key3']),
            ('incr', 'cython:counter'),
            ('incr', 'cython:counter'),
        ]

        if hasattr(redis, 'batch_operations'):
            print("‚úì Using Cython batch operations")
            batch_results = redis.batch_operations(batch_ops)
            print(f"‚úì Batch operations completed: {len(batch_results)} operations")
        else:
            print("‚ö†Ô∏è Cython batch operations not available, using fallback")

        # Test AMQP routing with Cython optimization
        print("\nüì® Testing Cython-optimized AMQP routing...")
        router = AMQPRouter(redis, namespace="cython_test")

        # Create test exchange
        router.create_exchange("cython_direct", "direct")
        router.bind_queue("cython_direct", "cython_queue1", "test.route")
        router.bind_queue("cython_direct", "cython_queue2", "other.route")

        # Test direct routing (should use Cython if available)
        if hasattr(redis, 'amqp_route_message'):
            print("‚úì Using Cython AMQP routing")
            routed = redis.amqp_route_message("cython_direct", "direct", "test.route", "msg123")
            print(f"‚úì Cython routing result: {routed}")
        else:
            print("‚ö†Ô∏è Cython AMQP routing not available")

        # Performance comparison
        print("\n‚ö° Performance comparison...")

        import time

        # Test basic operations speed
        start_time = time.time()
        for i in range(100):
            redis.set(f"perf:key{i}", f"value{i}")
            redis.get(f"perf:key{i}")
        basic_time = time.time() - start_time

        # Test batch operations speed
        start_time = time.time()
        batch_ops = []
        for i in range(100):
            batch_ops.extend([
                ('set', f"batch:key{i}", f"value{i}"),
                ('get', f"batch:key{i}")
            ])

        if hasattr(redis, 'batch_operations'):
            redis.batch_operations(batch_ops)
        batch_time = time.time() - start_time

        print(f"Basic operations: {basic_time:.4f}s")
        print(f"Batch operations: {batch_time:.4f}s")
        if hasattr(redis, 'batch_operations'):
            print(f"Speedup: {basic_time/batch_time:.1f}x")
    print("‚úì Cython performance optimizations example completed")


def redis_plugins_example():
    """Demonstrate Redis plugin/module loading and management"""
    print("\n=== Redis Plugins Example ===")

    with HighPerformanceRedis() as redis:
        # RedisPluginManager not implemented yet
        RedisPluginManager = None

        plugin_manager = RedisPluginManager(redis)

        # List available plugins
        print("üîç Discovering available plugins...")
        available = plugin_manager.list_available_plugins()
        print(f"‚úì Found {len(available)} available plugins: {list(available.keys())}")

        # List currently loaded modules
        print("\nüìã Currently loaded modules...")
        loaded_modules = plugin_manager.list_loaded_plugins()
        print(f"‚úì Loaded modules: {loaded_modules}")

        # Plugin compatibility checks
        print("\n‚úÖ Checking plugin compatibility...")
        for plugin_name in ['redisjson', 'redisearch', 'redisbloom']:
            compatibility = plugin_manager.validate_plugin_compatibility(plugin_name)
            status = "‚úÖ Compatible" if compatibility['compatible'] else "‚ùå Incompatible"
            print(f"‚úì {plugin_name}: {status}")
            if compatibility.get('warnings'):
                for warning in compatibility['warnings']:
                    print(f"  ‚ö†Ô∏è  {warning}")

        # Create plugin configurations
        print("\n‚öôÔ∏è  Creating plugin configurations...")
        plugin_manager.create_plugin_config('redisjson', {
            'max_depth': 10,
            'optimize_updates': True
        })

        plugin_manager.create_plugin_config('redisearch', {
            'max_expansions': 1000,
            'timeout': 5000
        })

        # Export/import configuration
        print("\nüíæ Exporting plugin configuration...")
        if 'redisjson' in plugin_manager.plugin_configs:
            config_json = plugin_manager.export_plugin_config('redisjson')
            print(f"‚úì Exported config ({len(config_json)} chars)")

            # Import it back
            success = plugin_manager.import_plugin_config(config_json)
            print(f"‚úì Import successful: {success}")

        # Plugin health monitoring
        print("\nüè• Monitoring plugin health...")
        health = plugin_manager.monitor_plugin_health()
        for plugin_name, status in health.items():
            print(f"‚úì {plugin_name}: {status['status']}")

        # Plugin backup/restore simulation
        print("\nüíæ Creating plugin backups...")
        for plugin_name in ['redisjson', 'redisearch']:
            backup = plugin_manager.create_plugin_backup(plugin_name)
            print(f"‚úì Backup created for {plugin_name}: {len(str(backup))} bytes")

        print("\n‚ö†Ô∏è  Note: Plugin loading examples require actual Redis module files.")
        print("   In a real environment, you would:")
        print("   1. Install Redis modules (redisjson, redisearch, etc.)")
        print("   2. Load them: plugin_manager.load_plugin('redisjson')")
        print("   3. Use module-specific commands through the Redis client")

        print("‚úì Redis plugins example completed")


def pg_cache_example():
    """
    Demonstrate PostgreSQL read-through caching with Redis module

    IMPORTANT: This requires the pgcache Redis module to be loaded.
    The example will attempt to load it automatically.
    """
    print("\n=== PostgreSQL Read-Through Cache Example (Redis Module) ===")

    try:
        # PostgreSQL cache features not implemented yet
        PGCacheManager = None
        load_pgcache_module = lambda redis: False
        check_pgcache_module_loaded = lambda redis: False
        get_pgcache_module_info = lambda redis: None

        with HighPerformanceRedis() as redis:
            # PostgreSQL configuration
            pg_config = {
                'host': 'localhost',
                'port': 5432,
                'database': 'testdb',
                'user': 'postgres',
                'password': 'password'
            }

            # Check if module is already loaded
            if not check_pgcache_module_loaded(redis):
                print("üîß Loading pgcache Redis module...")
                try:
                    success = load_pgcache_module(redis, pg_config)
                    if success:
                        print("‚úì pgcache module loaded successfully")
                    else:
                        print("‚úó Failed to load pgcache module")
                        return
                except Exception as e:
                    print(f"‚úó Error loading module: {e}")
                    print("Make sure PostgreSQL dependencies are available")
                    return
            else:
                print("‚úì pgcache module already loaded")

            # Get module info
            module_info = get_pgcache_module_info(redis)
            if module_info:
                print(f"‚úì Module version: {module_info.get('ver', 'unknown')}")

            # Cache configuration
            cache_config = {
                'cache_prefix': 'pg_cache:',
                'default_ttl': 300,  # 5 minutes
                'enable_pubsub': True,
                'pubsub_channel': 'pg_cache_events'
            }

            with PGCacheManager(redis, pg_config, cache_config) as cache_mgr:
                print("‚úì PostgreSQL cache manager initialized")

                # Example table operations (requires test table)
                # In a real scenario, you'd have tables like:
                # CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT, email TEXT);

                table_name = "users"

                try:
                    # Get user by ID (read-through cache via Redis module)
                    print("\nüìñ Testing read-through caching (Redis module)...")
                    user = cache_mgr.get(table_name, id=1)
                    if user:
                        print(f"‚úì Found user: {user}")
                    else:
                        print("‚ö†Ô∏è User not found (cache miss, Redis module queried PostgreSQL)")

                    # Get multiple users via Redis module
                    print("\nüìö Testing multi-get (Redis module)...")
                    users = cache_mgr.get_multi(table_name, [
                        {'id': 1},
                        {'id': 2},
                        {'id': 3}
                    ])
                    print(f"‚úì Retrieved {len([u for u in users if u])} users from cache/database")

                    # Direct Redis module commands
                    print("\nüîß Testing direct Redis module commands...")
                    try:
                        # Direct module call
                        result = redis.call_method('PGCACHE.READ', 'users', '{"id": 1}')
                        if result:
                            print(f"‚úì Direct module call result: {result}")
                        else:
                            print("‚ö†Ô∏è Direct module call returned null")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Direct module call failed: {e}")

                    # Setup automatic invalidation triggers
                    print("\nüîÑ Setting up auto-invalidation...")
                    cache_mgr.setup_auto_invalidation(table_name)
                    print("‚úì Automatic cache invalidation triggers installed")

                    # Preload table data
                    print("\n‚ö° Preloading table data...")
                    preloaded = cache_mgr.preload_table(table_name, "active = true")
                    print(f"‚úì Preloaded {preloaded} active records into cache")

                    # Get cache statistics
                    print("\nüìä Cache statistics...")
                    stats = cache_mgr.get_stats()
                    print(f"‚úì Cache stats: {stats}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Database operations failed (expected if PostgreSQL not configured): {e}")
                    print("   This example requires a running PostgreSQL instance with test data.")

                # Demonstrate async operations
                print("\nüöÄ Testing async operations...")
                import asyncio

                async def async_demo():
                    try:
                        # Async get operation
                        user = await cache_mgr.get_async(table_name, id=1)
                        if user:
                            print(f"‚úì Async retrieved user: {user['name'] if user else 'None'}")
                        else:
                            print("‚ö†Ô∏è Async user not found")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Async operation failed: {e}")

                asyncio.run(async_demo())

                # Demonstrate event subscription
                print("\nüì° Testing real-time event subscription...")

                async def event_demo():
                    event_count = {'received': 0}

                    def handle_event(event):
                        event_count['received'] += 1
                        print(f"üì® Received event: {event['type']} for table {event['table']}")

                    try:
                        async with cache_mgr.cache.subscribe_events(handle_event):
                            print("‚úì Listening for cache events (5 seconds)...")
                            await asyncio.sleep(2)  # Listen for 2 seconds

                            # Generate some events by doing cache operations
                            cache_mgr.get(table_name, id=999)  # Cache miss
                            cache_mgr.invalidate(table_name, id=1)  # Invalidation

                            await asyncio.sleep(1)  # Wait for events to propagate

                        print(f"‚úì Received {event_count['received']} cache events")

                    except Exception as e:
                        print(f"‚ö†Ô∏è Event subscription failed: {e}")

                asyncio.run(event_demo())

    except ImportError as e:
        print(f"‚ö†Ô∏è PostgreSQL cache dependencies not available: {e}")
        print("   Install required packages: pip install psycopg2-binary")

    print("‚úì PostgreSQL read-through cache example completed")


def lua_scripting_example():
    """Demonstrate Lua script management and debugging"""
    print("\n=== Lua Script Management Example ===")

    with HighPerformanceRedis() as redis:
        # Create script manager
        script_manager = LuaScriptManager(redis, "myapp:scripts")

        # Register a sample script
        rate_limit_script = """
        local key = KEYS[1]
        local limit = tonumber(ARGV[1])
        local window = tonumber(ARGV[2])

        local current = redis.call('INCR', key)
        if current == 1 then
            redis.call('EXPIRE', key, window)
        end

        return current <= limit
        """

        print("Registering rate limit script...")
        sha = script_manager.register_script("rate_limit", rate_limit_script, "1.0.0")
        print(f"‚úì Script registered with SHA: {sha}")

        # Execute the script
        result = script_manager.execute_script("rate_limit", ["rate:user:123"], ["10", "60"])
        print(f"‚úì Rate limit check result: {result}")

        # Validate a script
        invalid_script = "return redis.call('INVALID_COMMAND')"
        validation = script_manager.validate_script(invalid_script)
        print(f"‚úì Script validation: {validation['valid']}")

        # List registered scripts
        scripts = script_manager.list_scripts()
        print(f"‚úì Registered scripts: {list(scripts.keys())}")

        print("‚úì Lua scripting example completed")


def redis_clustering_example():
    """Demonstrate Redis Cluster management"""
    print("\n=== Redis Cluster Management Example ===")

    with HighPerformanceRedis() as redis:
        cluster_manager = RedisClusterManager(redis)

        # Get cluster topology
        print("Getting cluster topology...")
        topology = cluster_manager.get_cluster_topology()

        if topology.get('available', False):
            print(f"‚úì Cluster state: {topology.get('info', {}).get('cluster_state', 'unknown')}")
            print(f"‚úì Total nodes: {len(topology.get('nodes', []))}")
            print(f"‚úì Healthy: {topology.get('healthy', False)}")

            # Get slot distribution
            distribution = cluster_manager.get_slot_distribution()
            print(f"‚úì Slot distribution: {len(distribution)} nodes with slots")

            # Find node for a key
            key_location = cluster_manager.find_node_for_key("mykey")
            if key_location:
                print(f"‚úì Key 'mykey' maps to slot {key_location['slot']} on node {key_location['node']}")

            # Get comprehensive stats
            stats = cluster_manager.get_cluster_stats()
            print(f"‚úì Cluster stats retrieved for {stats.get('total_nodes', 0)} nodes")

        else:
            print("‚úó Redis Cluster not available (running standalone Redis?)")

        print("‚úì Redis clustering example completed")


def redis_sentinel_example():
    """Demonstrate Redis Sentinel management"""
    print("\n=== Redis Sentinel Management Example ===")

    with HighPerformanceRedis() as redis:
        sentinel_manager = RedisSentinelManager(redis)

        # Get sentinel overview
        print("Getting sentinel overview...")
        overview = sentinel_manager.get_sentinel_overview()

        if overview.get('available', False):
            print(f"‚úì Monitoring {overview.get('total_masters', 0)} masters")

            for master in overview.get('masters', []):
                print(f"  - Master '{master['name']}': {master['slaves_count']} slaves, {master['sentinels_count']} sentinels")

                # Get master address
                addr = sentinel_manager.get_master_address(master['name'])
                if addr:
                    print(f"    Address: {addr[0]}:{addr[1]}")

                # Get failover status
                status = sentinel_manager.get_failover_status(master['name'])
                if status.get('master_info'):
                    print(f"    Status: {status.get('failover_state', 'unknown')}")

        else:
            print("‚úó Redis Sentinel not available (running standalone Redis?)")

        print("‚úì Redis sentinel example completed")


def advanced_clustering_example():
    """Demonstrate advanced clustering features"""
    print("\n=== Advanced Clustering Features Example ===")

    with HighPerformanceRedis() as redis:
        print("üöÄ CyRedis - Advanced Clustering Features")
        print("=" * 50)

        # Test cluster commands (will work on cluster, gracefully fail on standalone)
        try:
            # Cluster info
            info = redis.cluster_info()
            print(f"‚úì Cluster info: {len(info)} bytes")

            # Key slot calculation
            slot = redis.cluster_keyslot("test:key:123")
            print(f"‚úì Key slot for 'test:key:123': {slot}")

            # Sentinel commands
            masters = redis.sentinel_masters()
            if masters:
                print(f"‚úì Sentinel monitoring {len(masters)} masters")
            else:
                print("! No sentinel masters (expected if not using sentinel)")

        except Exception as e:
            print(f"‚ö†Ô∏è Clustering commands failed (expected on standalone Redis): {e}")

        # Demonstrate slot analysis (works on any Redis)
        try:
            keys_in_slot = redis.cluster_getkeysinslot(0, 10)  # First 10 keys in slot 0
            print(f"‚úì Found {len(keys_in_slot)} keys in slot 0")
        except:
            print("‚ö†Ô∏è Slot analysis requires cluster mode")

        # Script management with clustering awareness
        script_manager = LuaScriptManager(redis)

        # Create a cluster-aware script
        cluster_script = """
        local key = KEYS[1]
        local value = ARGV[1]

        -- This script works in both cluster and standalone modes
        redis.call('SET', key, value)
        return redis.call('GET', key)
        """

        validation = script_manager.validate_script(cluster_script)
        if validation['valid']:
            print(f"‚úì Cluster-aware script validated (SHA: {validation['sha']})")

            # Register the script
            sha = script_manager.register_script("cluster_set", cluster_script, "1.0.0")
            print(f"‚úì Script registered for cluster use: {sha}")

        print("\nüéØ Advanced clustering features demonstrated!")
        print("‚úì Cluster topology management")
        print("‚úì Sentinel monitoring and failover")
        print("‚úì Slot distribution and key mapping")
        print("‚úì Cluster-aware script management")
        print("‚úì Cross-deployment compatibility")


def stream_consumer_callback(stream: str, message_id: str, data: Dict[str, Any]):
    """Callback function for stream consumer"""
    print(f"Consumed from {stream} [ID: {message_id}]: {data}")
    logger.info(f"Processing message {message_id} from {stream}")


def threaded_stream_consumer_example():
    """Demonstrate high-throughput stream consumption with threading"""
    print("\n=== Threaded Stream Consumer Example ===")

    with HighPerformanceRedis(max_workers=4) as redis:
        # Create a producer thread that adds messages to streams
        def producer_thread():
            stream_names = ["highload:stream1", "highload:stream2", "highload:stream3"]
            for i in range(50):  # Add 50 messages total
                stream = stream_names[i % len(stream_names)]
                redis.xadd(stream, {
                    "message_number": i,
                    "producer_thread": str(threading.get_ident()),
                    "timestamp": str(time.time()),
                    "data": f"High-performance message #{i}"
                })
                time.sleep(0.01)  # Small delay to simulate real workload
            print("Producer finished adding messages")

        # Start producer thread
        producer = threading.Thread(target=producer_thread, daemon=True)
        producer.start()

        # Create threaded stream consumer
        consumer = ThreadedStreamConsumer(redis, max_workers=4)

        # Subscribe to streams
        for stream_num in range(1, 4):
            stream_name = f"highload:stream{stream_num}"
            consumer.subscribe(stream_name, "0")
            consumer.set_callback(stream_name, stream_consumer_callback)

        # Start consuming
        consumer.start_consuming()

        # Let it run for a bit
        print("Consuming messages for 3 seconds...")
        time.sleep(3)

        # Stop consuming
        consumer.stop_consuming()

        # Wait for producer to finish
        producer.join(timeout=1.0)

        print("Stream consumer example completed")


def performance_comparison():
    """Compare performance between regular and threaded operations"""
    print("\n=== Performance Comparison ===")

    with HighPerformanceRedis(max_workers=8) as redis:
        num_operations = 100

        # Sequential operations
        start_time = time.time()
        for i in range(num_operations):
            redis.set(f"perf:key:{i}", f"value{i}")
        sequential_time = time.time() - start_time

        # Threaded operations
        start_time = time.time()
        futures = []
        for i in range(num_operations):
            future = redis.set_threaded(f"perf:key:{i}", f"value{i}")
            futures.append(future)

        # Wait for all threaded operations to complete
        for future in futures:
            future.result()
        threaded_time = time.time() - start_time

        print(".4f")
        print(".4f")
        print(".2f")

        # Clean up
        for i in range(num_operations):
            redis.delete(f"perf:key:{i}")


async def main():
    """Run all examples"""
    print("High-Performance Cython Redis Examples")
    print("=" * 50)

    try:
        # Basic operations
        basic_operations_example()

        # Async operations
        await async_operations_example()

        # Threaded operations
        threaded_operations_example()

        # Stream consumer
        threaded_stream_consumer_example()

        # Performance comparison
        performance_comparison()

        # Advanced features examples
        stream_consumer_group_example()
        worker_queue_example()
        pubsub_hub_example()
        keyspace_notifications_example()
        distributed_locks_example()
        local_cache_example()
        live_object_example()
        redisson_pro_features_example()
        lua_scripting_example()
        redis_clustering_example()
        redis_sentinel_example()
        advanced_clustering_example()
        asyncio.run(uvloop_async_example())
        lua_scripts_example()
        external_lua_scripts_example()
        redisson_pro_features_example()
        amqp_routing_example()
        cython_performance_example()
        redis_plugins_example()
        pg_cache_example()

        print("\nAll examples completed successfully!")

    except Exception as e:
        print(f"Error running examples: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Check if extension is built
    try:
        # CyRedisClient not implemented yet
        CyRedisClient = None
        print("CyRedis extension is available.")
    except ImportError:
        print("CyRedis extension not built. Building...")
        if build_extension():
            print("Extension built successfully. Please run the script again.")
        else:
            print("Failed to build extension. Please check dependencies.")
        exit(1)

    # Run examples
    asyncio.run(main())
